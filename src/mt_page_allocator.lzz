#hdr

#ifndef STT_TLS_WRAPPER
	#ifdef __WIN32
		#define STT_TLS_WRAPPER WindowsThreadLocalWrapper
		#define STT_WINDOWS_TLS 1
		#define STT_thread_local_PATL_Data int
		#define STT_DWORD DWORD
	#else
		#define STT_TLS_WRAPPER NativeThreadLocalWrapper
		#define STT_thread_local_PATL_Data thread_local PATL_Data
		#define STT_DWORD int
	#endif
#endif

namespace stt {
	struct PATL_Data;
	}
	
#include <mutex>
#include <atomic>
#end

#src
#end

namespace stt {
struct PATL_Data; // Page Allocacator Thread Local Data

////////////////////////////////////////////////////////////////////////
// wrapper around thread_local keyword (which is very broken in the wild)
struct NativeThreadLocalWrapper {
	// Use on platforms/compilers where "thread_local" keyword actually works
	static STT_thread_local_PATL_Data* mData;
	
	inline PATL_Data* getTlsData() { return mData; }
	inline void setTlsData(PATL_Data* ptr) { mData = ptr; }
	};
	
struct WindowsThreadLocalWrapper {
	// Implement the windows TLS api here
	STT_DWORD dwTlsIndex;
	
	WindowsThreadLocalWrapper() {
		dwTlsIndex = 0;
		#ifdef STT_WINDOWS_TLS
		dwTlsIndex = TlsAlloc();
		if (dwTlsIndex == TLS_OUT_OF_INDEXES)
			stt_abort();
		#endif
		}
	~WindowsThreadLocalWrapper() {
		#ifdef STT_WINDOWS_TLS
		TlsFree(dwTlsIndex);
		#endif
		}
	
	inline PATL_Data* getTlsData() {
		#ifdef STT_WINDOWS_TLS
		return (PATL_Data*) TlsGetValue(dwTlsIndex);
		#else
		return NULL;
		#endif
		}
	inline void setTlsData(PATL_Data* ptr) {
		#ifdef STT_WINDOWS_TLS
		TlsSetValue(dwTlsIndex, ptr);
		#endif
		}
	};

/*
struct T2ThreadLocalWrapper {
	inline PATL_Data* getTlsData() {
		TLS_Container* TD = TLS_Container::getData();
		return ((TLS_Entry<stt::PATL_Data*, 7>*) TD->dataArray[7])->data;
		}
	inline void setTlsData(PATL_Data* ptr) {
		TLS_Container* TD = TLS_Container::getData();
		((TLS_Entry<stt::PATL_Data*, 7>*) TD->dataArray[7])->data = ptr;
		}
	};
*/

////////////////////////////////////////////////////////////////////////
// The actual thread local data

		
////////////////////////////////////////////////////////////////////////
// The allocator

class ThreadSafePageAllocator {
	// Public facing api for the allocator
public:
	static void initThreadLocalAllocators() {
		// MUST be called on thread startup!
		ThreadSafePageAllocatorImpl::get().initThreadLocalAllocators();
		}
	static void cleanupThreadLocalAllocators() {
		// MUST be called on thread end!
		ThreadSafePageAllocatorImpl::get().cleanupThreadLocalAllocators();
		}
	static PATL_Data* getThreadLocalAllocators() {
		return ThreadSafePageAllocatorImpl::get().getThreadLocalAllocators();
		}
	
	// PageI
	static pageI* allocPage() {
		// Allocates a single pageI
		pageI* arr[1];
		ThreadSafePageAllocatorImpl::get().allocPages(&arr[0], 1);
		return arr[0];
		}
	static void freePage(pageI* page) {
		pageI* arr[1];
		arr[0] = page;
		ThreadSafePageAllocatorImpl::get().freePages(&arr[0], 1);
		}
	static void allocPages(pageI** pages, const uint32_t nPages) { return ThreadSafePageAllocatorImpl::get().allocPages(pages, nPages); }
	static void freePages(pageI** pages, const uint32_t nPages)  { return ThreadSafePageAllocatorImpl::get().freePages(pages, nPages); }
	};

class BackendPagePool {
public:
	// Can be accessed from multiple threads
	// This feeds pages from the system -> local pool -> thead_local pageAllocatorI's
	// allocation is locking but free'ing is lock free
	pageTypeEnum mPageType;
	uint32_t batchSize;	// when system allocation how many pages to alloc?
	
	// Locked alloc
	pageHeader* allocFreeList; // freeList of avaliable pages, used for allocations
	std::mutex mMutex;	// could be made lock free with an atomic bitmap
	
	// Multithreaded lock free return
	std::atomic<pageHeader*> _list; // used for deletions
	
	BackendPagePool() {
		mPageType = pageTypeEnum::PAGE_TYPE_UNSET;
		batchSize = 100;
		
		allocFreeList = NULL;
		_list = NULL;
		}
		
	~BackendPagePool() {
		#ifdef STT_DEBUG_PAGE
		printf("~BackendPagePool %s\n", pageTypeEnumToString(mPageType));
		dbg_print_status();
		#endif
		freeAll();
		}
		
	void dbg_print_status() {
		#ifdef STT_DEBUG_PAGE
		mMutex.lock();
		pageHeader* h = atomicTake();
			printf("\tBackendPagePool (%s): %p, %i -- %p, %i\n", pageTypeEnumToString(mPageType), allocFreeList, allocFreeList ? allocFreeList->listLength() : 0, h, h ? h->listLength() : 0);
		if (h) atomicMerge(h);
		mMutex.unlock();
		#endif
		}
	
	void freeAll() {
		// Takes all pages held here and deallocates them
		mMutex.lock();
		pageHeader* h = atomicTake();
		#ifdef STT_DEBUG_PAGE
		printf("BackendPagePool freeAll IN: h: %p, allocFreeList: %p\n", h, allocFreeList);
		#endif
		if (h) {
			if (allocFreeList)
				allocFreeList->appendList(h);
			else
				allocFreeList = h;
			}
		#ifdef STT_DEBUG_PAGE
		printf("BackendPagePool freeAll: allocFreeList %p\n", allocFreeList);
		#endif
		ThreadSafePageAllocatorImpl::systemFreeList(allocFreeList);
		allocFreeList = NULL;
		mMutex.unlock();
		}
	
	inline void atomicMerge(pageHeader* _insert, const uint32_t _nReturned) {
		#ifdef STT_DEBUG_PAGE
		printf("BackendPagePool atomicMerge: %p %i\n", _insert, _nReturned);
		#endif
		atomicMerge(_insert);
		}
		
	void atomicMerge(pageHeader* _insert) {
		// Adds @_insert to the atomic free list @this->_list
		
		// Remove head from atomic to working memory
		pageHeader* workingList = atomicTake();
		// _list is now NULL
		
		// merge the lists
		if (workingList)
			_insert->appendList(workingList);
		workingList = _insert;
		
		// replace NULL head with the working list
		pageHeader* nullList = NULL;
		if (!_list.compare_exchange_strong(nullList, workingList)) {
			// failed merge, this must have been due to pre-emption, re-merge
			atomicMerge(workingList);
			}
		}

	pageHeader* atomicTake () {
		// Reads the value of @_list and replaces it with NULL
		pageHeader* workingList = _list.load();
		while (!_list.compare_exchange_weak(workingList, NULL));
		return workingList; 
		}
	
	void bulkFetch(pageHeader** store, const uint32_t nPages) {
		// Yank off atomic list and merge the linked lists
		pageHeader* h = atomicTake();
		mMutex.lock();
		if (h) {
			if (allocFreeList)
				allocFreeList->appendList(h);
			else
				allocFreeList = h;
			}
		
		pageHeader* w = allocFreeList;
		uint32_t i = 0;
		if (w) {
			for (i = 0; i < nPages && w->next; ++i) {
				store[i] = w;
				w = w->next;
				}
			}
			
		const uint32_t nAllocated = i;
		if (i == nPages) {
			// cut the linked list here
			pageHeader* newHead = w->next;
			w->next = NULL;
			w->cachedWorkingEnd = NULL;
			if (newHead)
				newHead->cachedWorkingEnd = allocFreeList->cachedWorkingEnd;
			allocFreeList = newHead;
			mMutex.unlock();
			return;
			}
		
		// we have a parital linked list, we need system allocation
		pageHeader* remaining = NULL;
		pageHeader* leftovers = NULL;
		ThreadSafePageAllocatorImpl::systemAllocate(mPageType, batchSize + nPages - nAllocated, nPages - nAllocated, &remaining, &leftovers);
		
		for (; i < nPages; ++i) {
			store[i] = remaining;
			remaining = remaining->next;
			}
		
		// the entire free list has been consumed so we can dispose of it here
		allocFreeList = leftovers;
		
		#if STT_DEBUG_PAGE
		printf ("bulkFetch %s, %i, %i\n", pageTypeEnumToString(mPageType), batchSize + nPages - nAllocated, nPages - nAllocated);
		int cnt = 0;
		pageHeader* tail = allocFreeList->endCounting(cnt);
		printf ("allocFreeList %p %p %p %p %i\n", allocFreeList, allocFreeList->next, allocFreeList->cachedWorkingEnd, tail, cnt);
		#endif
		
		mMutex.unlock();
		}
	};

class ThreadLocalPagePool {
public:
	// Thread_local page allocator
	pageHeader* freelist;
	pageTypeEnum mPageType;
	int nPagesInFreeList;
	
	// tuning parameters
	int requestAmount;	// no pages avaliable? request this many
	int maxFreeListAmount; // more pages than this -> (send > requestAmount) back to main
	
	ThreadLocalPagePool() {
		freelist = NULL;
		mPageType = pageTypeEnum::PAGE_TYPE_UNSET;
		nPagesInFreeList = 0;
		
		requestAmount = 10;
		maxFreeListAmount = 20;
		}
		
	~ThreadLocalPagePool() {
		#ifdef STT_DEBUG_PAGE
			printf("~ThreadLocalPagePool\n");
			dbg_print_status();
		#endif
		returnPagesToGlobalPool(freelist, nPagesInFreeList);
		freelist = NULL;
		}
		
	void dbg_print_status() {
		#ifdef STT_DEBUG_PAGE
			printf("\tThreadLocalPagePool (%s): %p, %i/%i\n", pageTypeEnumToString(mPageType), freelist, freelist ? freelist->listLength() : 0, nPagesInFreeList);
		#endif
		}
	
	void allocPages(pageHeader** pages, const uint32_t nPages) {
		uint32_t count = 0;
		while (freelist && count < nPages) {
			pages[count] = freelist;
			count++;
			freelist = freelist->next;
			}
		if (count < nPages) {
			// we are out of pages, request pages from TSPA (locking)
			const uint32_t want = requestAmount + nPages - count;
			pageHeader* localStore[want];
			
			if (mPageType == pageTypeEnum::PAGE_TYPE_NORMAL)
				ThreadSafePageAllocatorImpl::get().PageGlobalFreeList.bulkFetch(&localStore[0], want);
			else if (mPageType == pageTypeEnum::PAGE_TYPE_JUMBO)
				ThreadSafePageAllocatorImpl::get().JumboGlobalFreeList.bulkFetch(&localStore[0], want);
			else
				abort();
			
			
			uint32_t idx = 0;
			for (;count < nPages; ++count) {
				pages[count] = localStore[idx];
				++idx;
				}
					
			#ifdef STT_DEBUG_PAGE
				stt_assert(freelist == NULL, "freelist is null");
			#endif
			freelist = localStore[nPages];
			nPagesInFreeList = requestAmount;
			
			#ifdef STT_DEBUG_PAGE
			printf("ThreadLocalPagePool allocPages:\n");
				dbg_print_status();
			#endif
			}
		}
		
	
	void freePages(pageHeader** pages, const uint32_t nPages) {
		// assembles pages into a linked list, then adds to the freelist
		if (!nPages) return;
		for (uint32_t i = 0; i < nPages-1; ++i) {
			pages[i]->next = pages[i+1];
			}
		pages[nPages-1]->next = NULL;
		pages[0]->cachedWorkingEnd = pages[nPages-1];
		freePagesList(pages[0], nPages);
		}
		
	void freePagesList(pageHeader* pagesLinkedList, const uint32_t knownCount = 0) {
		// frees an already prepared linked list of pages
		// if the number of pages is not known then knownCount 
		uint32_t realKnownCount = knownCount;
		if (!realKnownCount) {
			pageHeader* tmp = pagesLinkedList;
			while (tmp) {
				tmp = tmp->next;
				realKnownCount++;
				}
			}
		nPagesInFreeList += realKnownCount;	
		
		if (freelist)
			pagesLinkedList->appendList(freelist);
		freelist = pagesLinkedList;
		
		if (nPagesInFreeList > maxFreeListAmount) {
			pageHeader* returnList = freelist->splitList(requestAmount);
			const uint32_t nReturned = nPagesInFreeList - requestAmount;
			returnPagesToGlobalPool(returnList, nReturned);			
			}
		}
		
	void returnPagesToGlobalPool(pageHeader* returnList, const uint32_t nReturned) {
		if (!returnList) return;
		nPagesInFreeList -= nReturned;
		// Return pages to main cache
		if (mPageType == pageTypeEnum::PAGE_TYPE_NORMAL)
			ThreadSafePageAllocatorImpl::get().PageGlobalFreeList.atomicMerge(returnList, nReturned);
		else if (mPageType == pageTypeEnum::PAGE_TYPE_JUMBO)
			ThreadSafePageAllocatorImpl::get().JumboGlobalFreeList.atomicMerge(returnList, nReturned);
		else
			abort();
		}
	};

struct PATL_Data {
	ThreadLocalPagePool pageAlloc;
	ThreadLocalPagePool jumboPageAlloc;
	
	PATL_Data() {
		pageAlloc.mPageType = pageTypeEnum::PAGE_TYPE_NORMAL;
		jumboPageAlloc.mPageType = pageTypeEnum::PAGE_TYPE_JUMBO;
		}
		
	~PATL_Data() {
		#ifdef STT_DEBUG_PAGE
			printf("~PATL_Data\n");
		#endif
		}
	};
	
class ThreadSafePageAllocatorImpl {
	/// Allocates pageI's
public:
	STT_TLS_WRAPPER mTls; // Wraps the thread_local 
		
	BackendPagePool PageGlobalFreeList; // global free-list
	BackendPagePool JumboGlobalFreeList;
	
	ThreadSafePageAllocatorImpl() {
		PageGlobalFreeList.mPageType = pageTypeEnum::PAGE_TYPE_NORMAL;
		JumboGlobalFreeList.mPageType = pageTypeEnum::PAGE_TYPE_JUMBO;
		}
	~ThreadSafePageAllocatorImpl() {}
	
	static ThreadSafePageAllocatorImpl& get() {
		static ThreadSafePageAllocatorImpl Instance;
		return Instance;
		}
		
	void initThreadLocalAllocators() {
		mTls.setTlsData(new PATL_Data);
		}
	void cleanupThreadLocalAllocators() {
		PATL_Data* r = ThreadSafePageAllocatorImpl::get().mTls.getTlsData();
		if (r) delete r;
		mTls.setTlsData(NULL);
		}
	void cleanupGlobalFreeLists() {
		PageGlobalFreeList.freeAll();
		JumboGlobalFreeList.freeAll();
		}
	PATL_Data* getThreadLocalAllocators() {
		return mTls.getTlsData();
		}
	
	// Invokes the thread_local allocators
	void allocPages(pageI** pages, const uint32_t nPages) {
		PATL_Data* LA = getThreadLocalAllocators();
		LA->pageAlloc.allocPages((pageHeader**) pages, nPages);
		}
	void freePages(pageI** pages, const uint32_t nPages) {
		PATL_Data* LA = getThreadLocalAllocators();
		LA->pageAlloc.freePages((pageHeader**) pages, nPages);
		}
	void allocJumboPages(pageI** pages, const uint32_t nPages) {
		PATL_Data* LA = getThreadLocalAllocators();
		LA->jumboPageAlloc.allocPages((pageHeader**) pages, nPages);
		}
	void freeJumboPages(pageI** pages, const uint32_t nPages) {
		PATL_Data* LA = getThreadLocalAllocators();
		LA->jumboPageAlloc.freePages((pageHeader**) pages, nPages);
		}
			
	// Used as last resort
	static void systemAllocate(const pageTypeEnum mPageType, const uint32_t nPagesTotal, const uint32_t nSplit, pageHeader** groupA, pageHeader** groupB) {
		// Group A & B are pointers to pointers, NOT arrays of pointers
		if (mPageType == pageTypeEnum::PAGE_TYPE_NORMAL)
			systemAllocate_impl<pageI>(nPagesTotal, nSplit, groupA, groupB);
		else if (mPageType == pageTypeEnum::PAGE_TYPE_JUMBO)
			systemAllocate_impl<jumboPageI>(nPagesTotal, nSplit, groupA, groupB);
		else
			abort();
		}
		
	template <typename PAGE_TYPE>
	static void systemAllocate_impl(const uint32_t nPagesTotal, const uint32_t nSplit, pageHeader** groupA, pageHeader** groupB) {
		// Group A & B are pointers to pointers, NOT arrays of pointers
		// allocates at least nPagesTotal, and returns (nSplit) into linked list groupA and the rest into linked list groupB
		
		#ifdef STT_DEBUG_PAGE
			printf("Allocating %i (%i) pages\n", nPagesTotal, nSplit);
		#endif
		
		pageHeader* ph[nPagesTotal];
		ph[0] = (pageHeader*) new PAGE_TYPE();
		for (uint i = 1; i < nPagesTotal; ++i) {
			ph[i] = (pageHeader*) new PAGE_TYPE();
			ph[i-1]->next = ph[i];
			}
		
		// are we splitting?
		if (nSplit > 0) {
			ph[nSplit-1]->next = NULL;
			*groupA = ph[0];
			*groupB = ph[nSplit];
			
			ph[0]->cachedWorkingEnd = ph[nSplit-1];
			ph[nSplit]->cachedWorkingEnd = ph[nPagesTotal-1];
			}
		else {
			*groupA = NULL;
			*groupB = ph[0];
			ph[0]->cachedWorkingEnd = ph[nPagesTotal-1];
			}
		}
		
	static void systemFreeList(pageHeader* head) {
		#ifdef STT_DEBUG_PAGE
			uint32_t nPagesTotal = 0;
		#endif
		pageHeader* w = head;
		while (w) {
			pageHeader* n = w->next;
			delete w;
			w = n;
			#ifdef STT_DEBUG_PAGE
				nPagesTotal++;
			#endif
			}
			
		#ifdef STT_DEBUG_PAGE
			printf("Freeing %i pages\n", nPagesTotal);
		#endif
		}
	};
	
}
